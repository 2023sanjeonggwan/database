{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mEU3zRafHOw",
        "outputId": "a80b88b6-4b61-427d-a5e6-d86e4b2e8e91"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGdmDRQWedZQ",
        "outputId": "7b434d4b-23a2-4e25-93cf-52ce6a0482bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TF-IDF Vectorizing: 100%|██████████| 600/600 [00:00<00:00, 761.01it/s]\n",
            "100%|██████████| 600/600 [00:19<00:00, 31.21it/s]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Okt 객체 생성\n",
        "okt = Okt()\n",
        "\n",
        "def preprocess_text(text, stopwords):\n",
        "    # 한글을 제외한 문자 제거 (영어 및 숫자 제외)\n",
        "    text = re.sub('[^ㄱ-ㅣ가-힣\\s]', '', text)\n",
        "    # 불용어 제거\n",
        "    words = okt.morphs(text)\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# CSV 파일 불러오기\n",
        "df = pd.read_csv('/content/article_result_025.csv')\n",
        "\n",
        "# Article_Text 컬럼에 대한 전처리 수행\n",
        "stopwords = ['의', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '단독', '속보', '에서', '이다']\n",
        "df['Processed_Text'] = df['Article_Text'].apply(lambda x: preprocess_text(x, stopwords))\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the entire corpus\n",
        "corpus = df['Processed_Text'].tolist()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# tqdm을 사용하여 로딩 바 추가\n",
        "tqdm.pandas(desc=\"TF-IDF Vectorizing\")\n",
        "df['TFIDF_Vector'] = df['Processed_Text'].progress_apply(lambda x: vectorizer.transform([x]))\n",
        "\n",
        "# 상위 5개 키워드 추출\n",
        "top_keywords = []\n",
        "\n",
        "# tqdm을 사용하여 로딩 바 추가\n",
        "tqdm.pandas(desc=\"Extracting Keywords\")\n",
        "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    text_vectorized = row['TFIDF_Vector']\n",
        "    tfidf_scores = zip(vectorizer.get_feature_names_out(), text_vectorized.sum(axis=0).tolist()[0])\n",
        "    top_keywords.append([keyword for keyword, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:7]])\n",
        "\n",
        "# 상위 5개 키워드를 DataFrame에 추가\n",
        "df_keywords = pd.DataFrame(top_keywords, columns=['Keyword1', 'Keyword2', 'Keyword3', 'Keyword4', 'Keyword5', 'Keyword6', 'Keyword7'])\n",
        "\n",
        "# 기존 데이터프레임과 키워드 데이터프레임을 합치기\n",
        "result_df = pd.concat([df[['Link']], df_keywords], axis=1)\n",
        "\n",
        "# 새로운 CSV 파일로 저장\n",
        "result_df.to_csv('keywords_result_025.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Okt 객체 생성\n",
        "okt = Okt()\n",
        "\n",
        "def preprocess_text(text, stopwords):\n",
        "    # 한글을 제외한 문자 제거 (영어 및 숫자 제외)\n",
        "    text = re.sub('[^ㄱ-ㅣ가-힣\\s]', '', text)\n",
        "    # 불용어 제거\n",
        "    words = okt.morphs(text)\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# CSV 파일 불러오기\n",
        "df = pd.read_csv('/content/article_result_020.csv')\n",
        "\n",
        "# Article_Text 컬럼에 대한 전처리 수행\n",
        "stopwords = ['의', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '단독', '속보', '에서', '이다']\n",
        "df['Processed_Text'] = df['Article_Text'].apply(lambda x: preprocess_text(x, stopwords))\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the entire corpus\n",
        "corpus = df['Processed_Text'].tolist()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# tqdm을 사용하여 로딩 바 추가\n",
        "tqdm.pandas(desc=\"TF-IDF Vectorizing\")\n",
        "df['TFIDF_Vector'] = df['Processed_Text'].progress_apply(lambda x: vectorizer.transform([x]))\n",
        "\n",
        "# 상위 5개 키워드 추출\n",
        "top_keywords = []\n",
        "\n",
        "# tqdm을 사용하여 로딩 바 추가\n",
        "tqdm.pandas(desc=\"Extracting Keywords\")\n",
        "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    text_vectorized = row['TFIDF_Vector']\n",
        "    tfidf_scores = zip(vectorizer.get_feature_names_out(), text_vectorized.sum(axis=0).tolist()[0])\n",
        "    top_keywords.append([keyword for keyword, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:7]])\n",
        "\n",
        "# 상위 5개 키워드를 DataFrame에 추가\n",
        "df_keywords = pd.DataFrame(top_keywords, columns=['Keyword1', 'Keyword2', 'Keyword3', 'Keyword4', 'Keyword5', 'Keyword6', 'Keyword7'])\n",
        "\n",
        "# 기존 데이터프레임과 키워드 데이터프레임을 합치기\n",
        "result_df = pd.concat([df[['Link']], df_keywords], axis=1)\n",
        "\n",
        "# 새로운 CSV 파일로 저장\n",
        "result_df.to_csv('keywords_result_020.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuUxnvZ-i-cW",
        "outputId": "6804da3f-7d8a-490f-b583-7394ee82c73c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TF-IDF Vectorizing: 100%|██████████| 596/596 [00:00<00:00, 762.99it/s]\n",
            "100%|██████████| 596/596 [00:22<00:00, 26.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Okt 객체 생성\n",
        "okt = Okt()\n",
        "\n",
        "def preprocess_text(text, stopwords):\n",
        "    # 한글을 제외한 문자 제거 (영어 및 숫자 제외)\n",
        "    text = re.sub('[^ㄱ-ㅣ가-힣\\s]', '', text)\n",
        "    # 불용어 제거\n",
        "    words = okt.morphs(text)\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# CSV 파일 불러오기\n",
        "df = pd.read_csv('/content/article_result_023.csv')\n",
        "\n",
        "# Article_Text 컬럼에 대한 전처리 수행\n",
        "stopwords = ['의', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '단독', '속보', '에서', '이다']\n",
        "df['Processed_Text'] = df['Article_Text'].apply(lambda x: preprocess_text(x, stopwords))\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the entire corpus\n",
        "corpus = df['Processed_Text'].tolist()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# tqdm을 사용하여 로딩 바 추가\n",
        "tqdm.pandas(desc=\"TF-IDF Vectorizing\")\n",
        "df['TFIDF_Vector'] = df['Processed_Text'].progress_apply(lambda x: vectorizer.transform([x]))\n",
        "\n",
        "# 상위 5개 키워드 추출\n",
        "top_keywords = []\n",
        "\n",
        "# tqdm을 사용하여 로딩 바 추가\n",
        "tqdm.pandas(desc=\"Extracting Keywords\")\n",
        "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    text_vectorized = row['TFIDF_Vector']\n",
        "    tfidf_scores = zip(vectorizer.get_feature_names_out(), text_vectorized.sum(axis=0).tolist()[0])\n",
        "    top_keywords.append([keyword for keyword, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:7]])\n",
        "\n",
        "# 상위 5개 키워드를 DataFrame에 추가\n",
        "df_keywords = pd.DataFrame(top_keywords, columns=['Keyword1', 'Keyword2', 'Keyword3', 'Keyword4', 'Keyword5', 'Keyword6', 'Keyword7'])\n",
        "\n",
        "# 기존 데이터프레임과 키워드 데이터프레임을 합치기\n",
        "result_df = pd.concat([df[['Link']], df_keywords], axis=1)\n",
        "\n",
        "# 새로운 CSV 파일로 저장\n",
        "result_df.to_csv('keywords_result_023.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6vi4lFji-hm",
        "outputId": "acee0777-52d1-4665-f4f9-17c8a78151a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TF-IDF Vectorizing: 100%|██████████| 595/595 [00:00<00:00, 790.10it/s]\n",
            "100%|██████████| 595/595 [00:20<00:00, 28.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Okt 객체 생성\n",
        "okt = Okt()\n",
        "\n",
        "def preprocess_text(text, stopwords):\n",
        "    # 한글을 제외한 문자 제거 (영어 및 숫자 제외)\n",
        "    text = re.sub('[^ㄱ-ㅣ가-힣\\s]', '', text)\n",
        "    # 불용어 제거\n",
        "    words = okt.morphs(text)\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# CSV 파일 불러오기\n",
        "df = pd.read_csv('/content/article_result_028.csv')\n",
        "\n",
        "# Article_Text 컬럼에 대한 전처리 수행\n",
        "stopwords = ['의', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '단독', '속보', '에서', '이다']\n",
        "df['Processed_Text'] = df['Article_Text'].apply(lambda x: preprocess_text(x, stopwords))\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the entire corpus\n",
        "corpus = df['Processed_Text'].tolist()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# tqdm을 사용하여 로딩 바 추가\n",
        "tqdm.pandas(desc=\"TF-IDF Vectorizing\")\n",
        "df['TFIDF_Vector'] = df['Processed_Text'].progress_apply(lambda x: vectorizer.transform([x]))\n",
        "\n",
        "# 상위 5개 키워드 추출\n",
        "top_keywords = []\n",
        "\n",
        "# tqdm을 사용하여 로딩 바 추가\n",
        "tqdm.pandas(desc=\"Extracting Keywords\")\n",
        "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    text_vectorized = row['TFIDF_Vector']\n",
        "    tfidf_scores = zip(vectorizer.get_feature_names_out(), text_vectorized.sum(axis=0).tolist()[0])\n",
        "    top_keywords.append([keyword for keyword, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:7]])\n",
        "\n",
        "# 상위 5개 키워드를 DataFrame에 추가\n",
        "df_keywords = pd.DataFrame(top_keywords, columns=['Keyword1', 'Keyword2', 'Keyword3', 'Keyword4', 'Keyword5', 'Keyword6', 'Keyword7'])\n",
        "\n",
        "# 기존 데이터프레임과 키워드 데이터프레임을 합치기\n",
        "result_df = pd.concat([df[['Link']], df_keywords], axis=1)\n",
        "\n",
        "# 새로운 CSV 파일로 저장\n",
        "result_df.to_csv('keywords_result_028.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUlh9x0Si-mh",
        "outputId": "23bded0f-c21c-45ec-a036-56503c7845d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TF-IDF Vectorizing: 100%|██████████| 600/600 [00:00<00:00, 768.34it/s]\n",
            "100%|██████████| 600/600 [00:21<00:00, 27.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Okt 객체 생성\n",
        "okt = Okt()\n",
        "\n",
        "def preprocess_text(text, stopwords):\n",
        "    # Check if text is a valid string\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "\n",
        "    # 한글을 제외한 문자 제거 (영어 및 숫자 제외)\n",
        "    text = re.sub('[^ㄱ-ㅣ가-힣\\s]', '', text)\n",
        "\n",
        "    # 불용어 제거\n",
        "    words = okt.morphs(text)\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# CSV 파일 불러오기\n",
        "df = pd.read_csv('/content/article_result_032.csv')\n",
        "\n",
        "# Article_Text 컬럼에 대한 전처리 수행\n",
        "stopwords = ['의', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '단독', '속보', '에서', '이다']\n",
        "df['Processed_Text'] = df['Article_Text'].apply(lambda x: preprocess_text(x, stopwords))\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the entire corpus\n",
        "corpus = df['Processed_Text'].tolist()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# tqdm을 사용하여 로딩 바 추가\n",
        "tqdm.pandas(desc=\"TF-IDF Vectorizing\")\n",
        "df['TFIDF_Vector'] = df['Processed_Text'].progress_apply(lambda x: vectorizer.transform([x]))\n",
        "\n",
        "# 상위 5개 키워드 추출\n",
        "top_keywords = []\n",
        "\n",
        "# tqdm을 사용하여 로딩 바 추가\n",
        "tqdm.pandas(desc=\"Extracting Keywords\")\n",
        "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    text_vectorized = row['TFIDF_Vector']\n",
        "    tfidf_scores = zip(vectorizer.get_feature_names_out(), text_vectorized.sum(axis=0).tolist()[0])\n",
        "    top_keywords.append([keyword for keyword, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:7]])\n",
        "\n",
        "# 상위 5개 키워드를 DataFrame에 추가\n",
        "df_keywords = pd.DataFrame(top_keywords, columns=['Keyword1', 'Keyword2', 'Keyword3', 'Keyword4', 'Keyword5', 'Keyword6', 'Keyword7'])\n",
        "\n",
        "# 기존 데이터프레임과 키워드 데이터프레임을 합치기\n",
        "result_df = pd.concat([df[['Link']], df_keywords], axis=1)\n",
        "\n",
        "# 새로운 CSV 파일로 저장\n",
        "result_df.to_csv('keywords_result_032.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m1Jfa08i-rL",
        "outputId": "0a8c3ead-e78e-47d0-e169-9f3e115d946d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TF-IDF Vectorizing: 100%|██████████| 597/597 [00:01<00:00, 525.30it/s]\n",
            "100%|██████████| 597/597 [00:18<00:00, 32.44it/s]\n"
          ]
        }
      ]
    }
  ]
}